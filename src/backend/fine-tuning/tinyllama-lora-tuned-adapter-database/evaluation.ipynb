{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6df401c-c7c3-46c3-a72d-bfec31dcd2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b054f56-ff64-4187-8376-211b15d1b7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28742efa49144664a79b6c1253c4b5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd41d5c57ca424e97851954346a9fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d208d9483504f4cbb8d6f92262ab750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239eaccd30a042a79738130835f3360d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e6eda2c22040ddb92f6553df4fa87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963c741c7ff945d2b8e40fd20482ea00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82442ed21b9c40c69ee5a3cf9044d6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'\n",
    "adapter_path = './tinyllama-lora-tuned-adapter-database'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ").eval()\n",
    "\n",
    "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
    "tuned_model = tuned_model.merge_and_unload().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87bc6915-cda2-4727-a199-536b49eddc33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['instruction'], batch['response'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9545e0a4-87a2-4f10-bb15-5b4c8dfb83dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e28825bf8b84803a0a71dd83bf8216c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and tokenize the dataset\n",
    "eval_ds = load_dataset('json', data_files='data.jsonl')['train']\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['instruction', 'response'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abaea686-0a40-4694-8dae-831ab145c0f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e234782a-0da5-4b42-aa05-7339bda4dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute perplexity for the base and tuned models\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model):\n",
    "    losses = []\n",
    "    \n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return math.exp(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90ca3614-2d46-49bc-a7dd-a644d4bbbc6c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Perplexity: 1271441.69\n",
      "Tuned Model Perplexity: 1.03\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and print perplexity\n",
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a0386d-79da-4073-a78a-a07b2eb223c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "How does snapshot isolation prevent write–write conflicts?\n",
      "### Response:\n",
      "Snapshot isolation prevents write–write conflicts by ensuring that writes are executed in a consistent state across multiple transactions. When a transaction writes to a database, it creates a snapshot of the database state. The snapshot is stored in a separate memory location, which is then used by the transaction to write to the database. This ensures that the transaction's write operations are executed in a consistent state, regardless of the order in which the transactions are executed. This is achieved by using a shared memory model, where each transaction has its own copy of the database state, and the state is updated atomically. This ensures that the database state is consistent across all transactions, preventing write–write conflicts.\n",
      "### Instruction:\n",
      "How does snapshot isolation prevent write–write conflicts?\n",
      "### Response:\n",
      "Snapshot isolation provides each transaction with a consistent snapshot at start time; writes go into a private workspace and are only committed if no other concurrent transaction has modified the same data, thus preventing write–write conflicts.\n",
      "Binding mode determines when locks are acquired in a transaction. In “eager” binding, locks are taken at transaction start; in “lazy” binding, locks are acquired only when needed. Eager reduces deadlocks but increases contention.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import random\n",
    "\n",
    "raw_data = load_dataset(\"json\", data_files=\"data.jsonl\", split=\"train[:20]\")\n",
    "refs = raw_data['response']\n",
    "\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "raw_data['instruction'][2]\n",
    "\n",
    "# Generate an answer using the tuned model\n",
    "print(generate(base_model, raw_data['instruction'][2]))\n",
    "\n",
    "# Generate an answer using the tuned model\n",
    "print(generate(tuned_model, raw_data['instruction'][2]))\n",
    "\n",
    "# Print the reference answer for comparison\n",
    "print(refs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3b696-db8f-4661-b4bc-6848ff6df645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
